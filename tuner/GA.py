import os
import sys
import copy
import json
import logging
import argparse
from tqdm import tqdm

import torch
import datetime
import time
import numpy as np
import pandas as pd

import utils

sys.path.append('../')

from models.steps import (sinlge_fitness_function, twice_fitness_function, prepareForGA)
from models.dnn import RedisSingleDNN, RedisTwiceDNN

parser = argparse.ArgumentParser()
parser.add_argument('--target', type = str, default = '1', help='Target Workload')
parser.add_argument('--persistence', type = str, choices = ["RDB","AOF"], default = 'RDB', help='Choose Persistant Methods')
parser.add_argument('--topk', type = int, default=4,)
parser.add_argument('--path',type= str)
parser.add_argument('--num', type = str)
parser.add_argument('--n_pool',type = int, default = 64)
parser.add_argument('--n_generation', type=int, default=10000,)
parser.add_argument("--model_mode", type = str, default = 'single', help = "model mode")

args = parser.parse_args()

if not os.path.exists('save_knobs'):
    assert "Do this file after running main.py"

def mse_loss(target, predict):
    return np.array([(target[:,i]-predict[:,i])**2 for i in range(2)]).sum(axis=0)

def ATR_loss(default, predict, weight):
    return sum([((-1**i)*weight[i]*(predict[:,i]-default[:,i]))/default[:,i] for i in range(2)])


print("======================MAKE GA LOGGER====================")
logger, log_dir = utils.get_logger(os.path.join('./GA_logs'))
def generate_config(persistence, top_k_knobs, final_solution_pool):
    """ Generate config file according to the top_k_knobs and final_solution_pool which are generated by GA

    Args:
        top_k_knobs (list[str]): the name of top k knobs
        final_solution_pool (DataFrame): the final value of top k knobs
    
    Return:
        config file: contain the top k config instead of original config
    """
    initial_config = json.load(open(f'../data/{persistence.lower()}_knobs.json', 'r'))

    top_k_knobs_value = list(final_solution_pool.iloc[0, 0:args.topk])
    top_k_knobs_config = {value: top_k_knobs_value[i]
                        for i, value in enumerate(top_k_knobs)}
    memory_dict, active_dict = dict(), dict()
    for key, value in initial_config.items():
        if 'memory' in key:
            memory_dict[key] = value
        elif 'defrag' in key:
            active_dict[key] = value
        if value == 'yes' or value == 'no':
            if key in top_k_knobs_config.keys():
                if top_k_knobs_config[key] == 0:
                    top_k_knobs_config[key] = 'no'
                else:
                    top_k_knobs_config[key] = 'yes'
    policy_dict = ["volatile-lru", "allkeys-lru", "volatile-lfu", "allkeys-lfu", 
                "volatile-random", "allkeys-random", "volatile-ttl", "noeviction"]
    fsync_dict = ["always", "everysec", "no"]

    # the rule of activedefrag related config
    if 'activedefrag' in top_k_knobs_config.keys():
        if top_k_knobs_config['activedefrag'] == 'no':
            for key, value in active_dict.items():
                del initial_config[key]
                initial_config["#"+key] = value
        else:
            for key in active_dict.keys():
                if key in top_k_knobs_config.keys():
                    initial_config[key] = int(top_k_knobs_config[key])
                    del top_k_knobs_config[key]
    else:
        for key in active_dict.keys():
            if key in top_k_knobs_config.keys():
                initial_config[key] = int(top_k_knobs_config[key])
                del top_k_knobs_config[key]

    # the rule of maxmemory related config
    for key in memory_dict.keys():
        if key in top_k_knobs_config.keys():
            if key == 'maxmemory':
                if top_k_knobs_config[key] == 1 or top_k_knobs_config[key] == 2 or top_k_knobs_config[key] == 3:
                    top_k_knobs_config[key] = int(top_k_knobs_config[key])
                initial_config[key] = str(top_k_knobs_config[key]) + 'gb'
                del top_k_knobs_config[key]
            elif key == 'maxmemory-policy':
                policy_id = int(top_k_knobs_config[key])
                initial_config[key] = policy_dict[policy_id]
                del top_k_knobs_config[key]
            elif key == 'maxmemory-samples':
                initial_config[key] = int(top_k_knobs_config[key])
                del top_k_knobs_config[key]
        if 'maxmemory' not in top_k_knobs_config.keys():
            initial_config['maxmemory'] = '32mb' if args.target in range(1, 10) else '1gb'

    # deal with the rest knobs
    if persistence == 'RDB':
        for key, value in top_k_knobs_config.items():
            initial_config[key] = value if type(value) is str else int(value)
    elif persistence == 'AOF':
        for key, value in top_k_knobs_config.items():
            if key == 'appendfsync':
                initial_config[key] = fsync_dict[int(value)]
            elif key == 'auto-aof-rewrite-min-size':
                initial_config[key] = str(int(value)) + 'mb'
            else:
                initial_config[key] = value if type(value) is str else int(value)
    
    i = 0
    today = datetime.datetime.now()
    name = persistence+'-'+today.strftime('%Y%m%d')+'-'+'%02d'%i+'.conf'
    while os.path.exists(os.path.join('./GA_config/', name)):
        i += 1
        name = persistence+'-'+today.strftime('%Y%m%d')+'-'+'%02d'%i+'.conf'
    top_k_config_path = os.path.join('./GA_config/', name)
    
    with open("../data/init_config.conf", "r") as read_file:
        lines = read_file.readlines() + ['\n']
        with open(top_k_config_path, "a+") as write_file:
            write_file.writelines(lines)
            for key, value in initial_config.items():
                if 'sec' in key or 'changes' in key:
                    line += f' {value}'
                    i += 1
                    if i < 2: 
                        continue
                    else:
                        write_file.write(line + '\n')
                else:
                    line = f'{key} ' + f'{value}'
                    write_file.write(line + '\n')
                i, line = 0, 'save'
    
    return top_k_config_path, name

def server_connection(top_k_config_path, name):
    import paramiko

    client = paramiko.SSHClient()
    client.set_missing_host_key_policy(paramiko.AutoAddPolicy())
    client.connect('34.64.145.240', username='jieun', password='1423')

    sftp = client.open_sftp()
    sftp.put(top_k_config_path, './redis-sample-generation/'+name)
    _, ssh_stdout, _ = client.exec_command('python ./redis-sample-generation/connection.py rdb ./redis-sample-generation/'+name)
    exit_status = ssh_stdout.channel.recv_exit_status()
    if exit_status == 0:
        sftp.get('/home/jieun/result_rdb_external_default_18.csv', './GA_config/result_rdb_external_default_18.csv')
        sftp.get('/home/jieun/result_rdb_internal_default_18.csv', './GA_config/result_rdb_internal_default_18.csv')
    sftp.close()
    client.exec_command('rm ./redis-sample-generation/'+name)
    client.exec_command('rm /home/jieun/result_rdb_external_default_18.csv')
    client.exec_command('rm /home/jieun/result_rdb_internal_default_18.csv')

    client.close()

def main():
    top_k_knobs = np.load(os.path.join('./save_knobs',args.path,f"knobs_{args.topk}.npy"))

    if args.model_mode == 'single':
        model = RedisSingleDNN(args.topk+5,2)
        model.load_state_dict(torch.load(os.path.join('./model_save',args.path,'model_{}.pt'.format(args.num))))
        fitness_function = sinlge_fitness_function

    if args.model_mode == 'twice':
        model = RedisTwiceDNN(args.topk+5,2)
        model.load_state_dict(torch.load(os.path.join('./model_save',args.path,'model_{}.pt'.format(args.num))))
        fitness_function = twice_fitness_function

    pruned_configs, external_data, default, scaler_X, scaler_y = prepareForGA(args,top_k_knobs)
    temp_configs = pd.concat([pruned_configs, external_data], axis=1)
    temp_configs = temp_configs.sort_values(["Totals_Ops/sec","Totals_p99_Latency"], ascending=[False,True])
    target = temp_configs[["Totals_Ops/sec","Totals_p99_Latency"]].values[0]
    configs = temp_configs.drop(columns=["Totals_Ops/sec","Totals_p99_Latency"])

    n_configs = top_k_knobs.shape[0]
    n_pool_half = args.n_pool//2
    mutation = int(n_configs*0.7)
    current_solution_pool = configs[:args.n_pool].values
    target = np.repeat([default], args.n_pool, axis = 0)

    for i in tqdm(range(args.n_generation)):
        scaled_pool = scaler_X.transform(current_solution_pool)
        predicts = fitness_function(scaled_pool, args, model)
        fitness = scaler_y.inverse_transform(predicts)
        idx_fitness = ATR_loss(target, fitness,(0.5,0.5))
        sorted_idx_fitness = np.argsort(idx_fitness)[n_pool_half:]
        best_solution_pool = current_solution_pool[sorted_idx_fitness,:]
        if i % 1000 == 999:
            logger.info(f"[{i+1:3d}/{args.n_generation:3d}] best fitness: {max(idx_fitness)}")
        pivot = np.random.choice(np.arange(1,n_configs))
        new_solution_pool = np.zeros_like(best_solution_pool)
        for j in range(n_pool_half):
            new_solution_pool[j][:pivot] = best_solution_pool[j][:pivot]
            new_solution_pool[j][pivot:n_configs] = best_solution_pool[n_pool_half-1-j][pivot:n_configs]
            new_solution_pool[j][n_configs:] = current_solution_pool[0][n_configs:]
            import utils, random
            random_knobs = utils.make_random_option(top_k_knobs)
            knobs = list(random_knobs.values())
            random_knob_index = list(range(n_configs))
            random.shuffle(random_knob_index)
            random_knob_index = random_knob_index[:mutation]
            for k in range(len(random_knob_index)):
                new_solution_pool[j][random_knob_index[k]] = knobs[random_knob_index[k]]
        current_solution_pool = np.vstack([best_solution_pool, new_solution_pool])
    final_solution_pool = pd.DataFrame(best_solution_pool)
    print(top_k_knobs)
    print(final_solution_pool)
    top_k_config_path, name = generate_config(args.persistence, top_k_knobs, final_solution_pool)
    server_connection(top_k_config_path, name)

if __name__ == '__main__':
    try:
        main()
    except:
        logger.exception("ERROR")
    finally:
        logger.handlers.clear()
        logging.shutdown()
